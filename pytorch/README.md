#### Some experience in training Transformer language model
- Use weight decay of 0.01 on non-bias, non-gamma and beta parameters to stabilize training.